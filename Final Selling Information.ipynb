{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import \n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Set the executable path and initialize Splinter\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Driver [/Users/xinghuili/.wdm/drivers/chromedriver/mac64/93.0.4577.15/chromedriver] found in cache\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def basic():\n",
    "    info = []\n",
    "    attributes = soup.find('div', class_= 'introContent')\n",
    "    basic = attributes.find('div', class_ = 'content')\n",
    "    transaction = attributes.find('div', class_ = 'transaction') \n",
    "    ul_basic = basic.find('ul')\n",
    "    ul_transaction = transaction.find('ul')\n",
    "    for li_basic in ul_basic.find_all('li'):\n",
    "        info_basic = li_basic.text[4:]\n",
    "        info.append(info_basic)\n",
    "    for li_transaction in ul_transaction.find_all('li'):\n",
    "        info_transaction = li_transaction.text[5:]\n",
    "        info.append(info_transaction)\n",
    "        info = [x.strip() for x in info if x.strip() !='']\n",
    "    return info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def price_and_year_built():\n",
    "    totalprice = soup.find('div', class_='price').span.text\n",
    "    unitprice = soup.find('div', class_ = 'unitPrice').span.text\n",
    "    year_built = soup.find('div', class_ = 'subInfo noHidden').text\n",
    "    return [totalprice, unitprice, year_built]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def name_and_area_code():\n",
    "    around_information = soup.find('div', class_ = 'aroundInfo')\n",
    "    name = around_information.find('div', class_ = 'communityName').a.text\n",
    "    area = around_information.find('span', class_ = 'info').a.text\n",
    "    code_information = soup.find('div', class_ = 'houseRecord')\n",
    "    code = code_information.find('span', class_ ='info').text[0:12]\n",
    "    return [name, area, code]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def merge():\n",
    "    final_info = name_and_area_code() + price_and_year_built() + basic()\n",
    "    return final_info"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Import Links to array \n",
    "links = pd.read_csv('Resources/Shanghai Selling Links.csv')\n",
    "links_array = links.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# iterate\n",
    "final= []\n",
    "header = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}\n",
    "for j in range(0,len(links_array)):\n",
    "    URL = links_array[j][0]\n",
    "    try:\n",
    "        r = requests.get(URL, headers = header, timeout=None)  \n",
    "        browser.visit(URL)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        final.append(merge())\n",
    "        print(j)\n",
    "    except Exception:\n",
    "        print('Timeout')\n",
    "browser.quit()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final = pd.DataFrame(final)\n",
    "final.to_csv('Resources/Shanghai Selling Information.csv', index = False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "61b2890ca4fe8716dc576c12343ff08f33079c09cc57cb5484f02a28f574fd89"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}